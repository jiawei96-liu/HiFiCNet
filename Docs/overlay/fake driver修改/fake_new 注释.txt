# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
# Copyright (c) 2010 Citrix Systems, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
A fake (in-memory) hypervisor+api.

Allows nova testing w/o a hypervisor.  This module also documents the
semantics of real hypervisor connections.

"""


# netaddr是一个Python库，用于处理和操作网络地址。它提供了一组功能强大且易于使用的类和函数，用于处理 IPv4、IPv6、MAC地址和CIDR块等网络地址，以及进行各种操作，如解析、分配、比较、分类和转换等。它还支持IP地址和CIDR块的网络计算，例如子网掩码和广播地址的计算。netaddr库的使用非常广泛，特别是在网络和安全领域中，它可以帮助用户轻松处理和管理大量的网络地址。
import netaddr
import collections
import contextlib
import time
import uuid

import fixtures
import os_resource_classes as orc
from oslo_log import log as logging
from oslo_serialization import jsonutils
from oslo_utils import versionutils

from nova.compute import power_state
from nova.compute import task_states
from nova.compute import vm_states
import nova.conf
from nova.console import type as ctype
from nova import context as nova_context
from nova import exception
from nova import objects
from nova.objects import diagnostics as diagnostics_obj
from nova.objects import fields as obj_fields
from nova.objects import migrate_data
from nova.virt import driver
from nova.virt import hardware
from nova.virt.ironic import driver as ironic
from nova.virt import virtapi

# oslo_concurrency是OpenStack的一个公共库，提供了一些并发编程的工具和工具函数，包括锁、信号、进程、协程等等。其中的processutils模块提供了一些方便的函数，用于跨平台地调用外部进程和shell命令，并处理它们的输出和错误等。
from oslo_concurrency import processutils
# nova是OpenStack中的一个项目，用于提供计算资源管理服务，包括虚拟机实例的创建、启动、停止、删除等等。其中的utils模块是一个通用的工具模块，提供了多个实用函数，用于在Nova项目中的各个模块中进行共享，例如日志记录、配置参数处理、异常处理、时间计算等等。这个模块中的函数和类可帮助项目实现更加高效和可靠的计算资源管理服务。
from nova import utils
# re是Python的内置正则表达式库。它提供了一组函数和方法，用于处理和操作正则表达式，例如匹配、替换、搜索等等。在OpenStack项目中，正则表达式通常用于处理和解析配置文件、日志文件等文本数据，以及进行各种数据验证和过滤等。
import re

CONF = nova.conf.CONF

LOG = logging.getLogger(__name__)

# 下面几个函数都使用了Nova项目中的common.privsep模块提供的sys_admin_pctxt.entrypoint装饰器，以便在特权进程上下文中运行。这些函数主要是用于管理Linux虚拟网络设备和路由表，以便在OpenStack计算节点环境中创建、删除和配置虚拟网络。


# 一个通用的命令执行包装器，用于记录和日志命令的执行情况，并在命令执行失败时抛出异常。该函数使用processutils.execute函数调用实际的命令执行，并返回其结果。
def execute_wrapper(args):
    LOG.info(
        "fake running command: %s",
        " ".join(str(arg) for arg in args),
    )
    #root_helper = utils.get_root_helper()
    try:
        return processutils.execute(*args)
    except Exception as e:
        LOG.error(
            "fake Unable to execute %(cmd)s. Exception: %(exception)s",
            {"cmd": args, "exception": e},
        )
        raise

# 用于在Linux系统中添加一个新的网络命名空间。它通过执行ip netns add命令来实现这个功能。该函数将ns参数作为新命名空间的名称，并使用execute_wrapper函数来执行该命令。
@nova.privsep.sys_admin_pctxt.entrypoint
def add_namespace(ns):
    full_args = ["ip", "netns", "add", ns]
    execute_wrapper(full_args)
    full_args = ["ip", "netns", "exec", ns, "ip", "link", "set", "lo", "up"]
    execute_wrapper(full_args)

# 用于删除指定的网络命名空间，它通过执行ip netns del命令来实现这个功能，并使用execute_wrapper函数来执行该命令。
@nova.privsep.sys_admin_pctxt.entrypoint
def delete_namespace(ns):
    # deleting namespace will delete its ports and veth pairs
    full_args = ["ip", "netns", "del", ns]
    execute_wrapper(full_args)

# 用于将IP地址添加到指定的端口中。它使用ip netns exec命令将命令传递给指定的命名空间，并使用execute_wrapper函数执行该命令。
@nova.privsep.sys_admin_pctxt.entrypoint
def add_port_ip_addresses(ns, ovs_port, ip_addresses):
    for address in ip_addresses:
        full_args = ["ip", "netns", "exec", ns,
                     "ip", "addr", "add", address, "dev", ovs_port]
        execute_wrapper(full_args)

# 用于在Linux系统中添加一个新的虚拟网络接口，并将其连接到指定的Linux桥接设备上。它使用`ovs-vsctl命令创建一个新的虚拟网络接口，并使用ip link set命令将它移动到指定的网络命名空间中。该函数还使用execute_wrapper`函数来执行这些命令。
@nova.privsep.sys_admin_pctxt.entrypoint
def add_port(ns, bridge, ovs_port, port_id, mac_address):
    full_args = ["ovs-vsctl", "--may-exist", "add-port", bridge, ovs_port,
                 "--", "set", "Interface", ovs_port, "type=internal",
                 "--", "set", "Interface", ovs_port,
                 "external_ids:iface-id=%s" % port_id,
                 "--", "set", "Interface", ovs_port,
                 "external-ids:iface-status=active",
                 "--", "set", "Interface", ovs_port,
                 "external-ids:attached-mac=%s" % mac_address]
    execute_wrapper(full_args)
    full_args = ["ip", "link", "set", ovs_port, "netns", ns]
    execute_wrapper(full_args)
    namespace = ["ip", "netns", "exec", ns]
    full_args = namespace + ["ip", "link", "set", ovs_port, "up"]
    execute_wrapper(full_args)
    namespace = ["ip", "netns", "exec", ns]
    full_args = namespace + ["ip", "link", "set", ovs_port, "address", mac_address]
    execute_wrapper(full_args)

# 用于删除指定的虚拟网络接口。它使用ovs-vsctl命令来删除指定的虚拟网络接口，并使用execute_wrapper函数执行该命令。
@nova.privsep.sys_admin_pctxt.entrypoint
def delete_port(ns, bridge, ovs_port):
    full_args = ["ovs-vsctl", "--if-exists", "del-port", bridge, ovs_port]
    execute_wrapper(full_args)

# 用于向指定的网络命名空间添加路由。它通过执行ip netns exec命令将命令传递给指定的命名空间，并使用execute_wrapper函数执行该命令。在执行时，该函数使用正则表达式提取IP地址中的子网掩码，并根据子网掩码计算网关地址。然后，该函数使用ip route add命令添加新路由，并使用execute_wrapper函数执行该命令。
@nova.privsep.sys_admin_pctxt.entrypoint
def add_route(ns, ip_addresses):
    namespace = ["ip", "netns", "exec", ns]
    for address in ip_addresses:
        m=re.search(r'(\d+).(\d+).(\d+).\d+/(\d+)',address)
        if m.group(4)=="8":
            ip=m.group(1)+".0.0.1"
        elif m.group(4)=="16":
            ip=m.group(1)+"."+m.group(2)+".0.1"
        elif m.group(4)=="24":
            ip=m.group(1)+"."+m.group(2)+"."+m.group(3)+".1"
        full_args=namespace+["route","add","default","gw",ip]
        execute_wrapper(full_args)

class FakeInstance(object):

    def __init__(self, name, state, uuid):
        LOG.info("---fake---instance_init---")
        self.name = name
        self.state = state
        self.uuid = uuid

    def __getitem__(self, key):
        return getattr(self, key)


class Resources(object):
    vcpus = 0
    memory_mb = 0
    local_gb = 0
    vcpus_used = 0
    memory_mb_used = 0
    local_gb_used = 0

    def __init__(self, vcpus=8, memory_mb=8000, local_gb=500):
        LOG.info("---fake---resource_init----")
        self.vcpus = vcpus
        self.memory_mb = memory_mb
        self.local_gb = local_gb

    def claim(self, vcpus=0, mem=0, disk=0):
        self.vcpus_used += vcpus
        self.memory_mb_used += mem
        self.local_gb_used += disk

    def release(self, vcpus=0, mem=0, disk=0):
        self.vcpus_used -= vcpus
        self.memory_mb_used -= mem
        self.local_gb_used -= disk

    def dump(self):
        return {
            'vcpus': self.vcpus,
            'memory_mb': self.memory_mb,
            'local_gb': self.local_gb,
            'vcpus_used': self.vcpus_used,
            'memory_mb_used': self.memory_mb_used,
            'local_gb_used': self.local_gb_used
        }


class FakeDriver(driver.ComputeDriver):
    # These must match the traits in
    # nova.tests.functional.integrated_helpers.ProviderUsageBaseTestCase
    capabilities = {
        "has_imagecache": True,
        "supports_evacuate": True,
        "supports_migrate_to_same_host": False,
        "supports_attach_interface": True,
        "supports_device_tagging": True,
        "supports_tagged_attach_interface": True,
        "supports_tagged_attach_volume": True,
        "supports_extend_volume": True,
        "supports_multiattach": True,
        "supports_trusted_certs": True,
        "supports_pcpus": False,
        "supports_accelerators": True,
        "supports_remote_managed_ports": True,

        # Supported image types
        "supports_image_type_raw": True,
        "supports_image_type_vhd": False,
        }

    # Since we don't have a real hypervisor, pretend we have lots of
    # disk and ram so this driver can be used to test large instances.
    vcpus = 1000
    memory_mb = 800000
    local_gb = 600000

    """Fake hypervisor driver."""

    def __init__(self, virtapi, read_only=False):
        LOG.info("------fakedriver_init------")
        super(FakeDriver, self).__init__(virtapi)
        self.instances = {}
        self.resources = Resources(
            vcpus=self.vcpus,
            memory_mb=self.memory_mb,
            local_gb=self.local_gb)
        self.host_status_base = {
            'hypervisor_type': 'fake',
            'hypervisor_version': versionutils.convert_version_to_int('1.0'),
            'hypervisor_hostname': CONF.host,
            'cpu_info': {},
            'disk_available_least': 0,
            'supported_instances': [(
                obj_fields.Architecture.X86_64,
                obj_fields.HVType.FAKE,
                obj_fields.VMMode.HVM)],
            'numa_topology': None,
          }
        self._mounts = {}
        self._interfaces = {}
        self.active_migrations = {}
        self._host = None
        self._nodes = None

    def init_host(self, host):
        self._host = host
        # NOTE(gibi): this is unnecessary complex and fragile but this is
        # how many current functional sample tests expect the node name.
        self._nodes = (['fake-mini'] if self._host == 'compute'
                       else [self._host])

    def _set_nodes(self, nodes):
        # NOTE(gibi): this is not part of the driver interface but used
        # by our tests to customize the discovered nodes by the fake
        # driver.
        self._nodes = nodes

    def list_instances(self):
        return [self.instances[uuid].name for uuid in self.instances.keys()]

    def list_instance_uuids(self):
        return list(self.instances.keys())





# 用于从虚拟网络中获取IP地址列表。它首先获取vif参数中的虚拟网络信息，然后迭代每个子网，提取IPv4地址，并将其转换为CIDR格式。最后，该函数返回一个IP地址列表。
    def get_ip_addresses(self, vif):
        addresses = []
        network = vif.get("network", {})
        for subnet in network.get("subnets", []):
            if subnet and subnet.get("version", "") == 4:
                cidr = subnet.get("cidr", None)
                for ip in subnet.get("ips", []):
                    ip_address = ip.get("address", None)
                    if cidr and ip_address:
                        prefixlen = netaddr.IPNetwork(cidr).prefixlen
                        ip_address = "%s/%s" % (ip_address, prefixlen)
                        addresses = addresses + [ip_address]
        return addresses

# 用于将虚拟网络接口连接到虚拟机实例中。它首先获取虚拟网络接口的设备名称、端口ID和MAC地址等信息，然后创建一个新的网络命名空间，并将虚拟网络接口移动到该命名空间中。接下来，该函数使用get_ip_addresses函数获取虚拟网络接口的IP地址列表，并使用add_port_ip_addresses函数将这些地址添加到虚拟网络接口中。最后，该函数使用add_route函数将这些地址添加到虚拟网络命名空间的路由表中，以便虚拟机实例可以通过这些地址进行通信。
    def plug_vif(self, instance, vif):
        bridge = "br-int"
        dev = vif.get("devname")
        port = vif.get("id")
        mac_address = vif.get("address")
        if not dev or not port or not mac_address:
            return
        ns = "fake-%s" % instance.uuid
        add_port(ns, bridge, dev, port, mac_address)
        ip_addresses = self.get_ip_addresses(vif)
        add_port_ip_addresses(ns, dev, ip_addresses)
        add_route(ns,ip_addresses)

# 首先创建一个以虚拟机实例的唯一标识符为名称的 Linux 命名空间，然后遍历 network_info 列表中的每个 VIF，调用 plug_vif 方法将该 VIF 插入虚拟机实例的网络中。注意，在遍历 network_info 列表之前，该方法打印一条日志消息 "fake-plug_vifs---TAG0"，如果 network_info 为 None，则打印一条日志消息 "fake-plug_vifs---TAG1"，否则打印一条日志消息 "fake-plug_vifs---TAG2"。在遍历 network_info 列表时，对于每个 VIF，该方法打印一条日志消息 "fake-plug_vifs---TAG3"，然后调用 plug_vif 方法插入该 VIF。
    def plug_vifs(self, instance, network_info):
        """Plug VIFs into networks."""
        LOG.info("---fake-plug_vifs---TAG0")
        ns = "fake-%s" % instance.uuid
        add_namespace(ns)
        if network_info == None:
            LOG.info("---fake-plug_vifs---TAG1")
        else:
            LOG.info("---fake-plug_vifs---TAG2")
        for vif in network_info:
            LOG.info("---fake-plug_vifs---TAG3")
            self.plug_vif(instance, vif)

# 从虚拟机实例的网络中移除一个特定的 VIF。这个方法会首先从 VIF 中获取设备名称和端口 ID，然后使用这些信息在 br-int 网桥上删除该设备。如果设备名称不存在，则会尝试使用端口 ID 来构造设备名称，如果端口 ID 不存在，则该方法不执行任何操作。
    def unplug_vif(self, instance, vif):
        bridge = "br-int"
        dev = vif.get("devname")
        port = vif.get("id")
        if not dev:
            if not port:
                return
            dev = "tap" + str(port[0:11])
        ns = "fake-%s" % instance.uuid
        delete_port(ns, bridge, dev)

# unplug_vifs 方法会从虚拟机实例的网络中移除所有的 VIF。它遍历 network_info 列表中的每个 VIF，并调用 unplug_vif 方法将该 VIF 从虚拟机实例的网络中移除。最后，它删除之前创建的 Linux 命名空间，以确保在删除 VIF 之后，命名空间也被删除。
    def unplug_vifs(self, instance, network_info):
        """Unplug VIFs from networks."""
        for vif in network_info:
            self.unplug_vif(instance, vif)
        # delete namespace after removing ovs ports
        ns = "fake-%s" % instance.uuid
        delete_namespace(ns)

    def spawn(self, context, instance, image_meta, injected_files,
              admin_password, allocations, network_info=None,
              block_device_info=None, power_on=True, accel_info=None):
        LOG.info("---fake-spawn---TAG0")
        self.plug_vifs(instance, network_info)
        if network_info:
            LOG.info("---fake-spawn---TAG2 {}".format(network_info))
            for vif in network_info:
                # simulate a real driver triggering the async network
                # allocation as it might cause an error
                vif.fixed_ips()
                # store the vif as attached so we can allow detaching it later
                # with a detach_interface() call.
                self._interfaces[vif['id']] = vif

        uuid = instance.uuid
        state = power_state.RUNNING if power_on else power_state.SHUTDOWN
        flavor = instance.flavor
        self.resources.claim(
            vcpus=flavor.vcpus,
            mem=flavor.memory_mb,
            disk=flavor.root_gb)
        fake_instance = FakeInstance(instance.name, state, uuid)
        self.instances[uuid] = fake_instance

    def snapshot(self, context, instance, image_id, update_task_state):
        if instance.uuid not in self.instances:
            raise exception.InstanceNotRunning(instance_id=instance.uuid)
        update_task_state(task_state=task_states.IMAGE_UPLOADING)

    def reboot(self, context, instance, network_info, reboot_type,
               block_device_info=None, bad_volumes_callback=None,
               accel_info=None):
        # If the guest is not on the hypervisor and we're doing a hard reboot
        # then mimic the libvirt driver by spawning the guest.
        if (instance.uuid not in self.instances and
                reboot_type.lower() == 'hard'):
            injected_files = admin_password = allocations = None
            self.spawn(context, instance, instance.image_meta, injected_files,
                       admin_password, allocations,
                       block_device_info=block_device_info)
        else:
            # Just try to power on the guest.
            self.power_on(context, instance, network_info,
                          block_device_info=block_device_info)

    def get_host_ip_addr(self):
        return '192.168.0.1'

    def set_admin_password(self, instance, new_pass):
        pass

    def resume_state_on_host_boot(self, context, instance, network_info,
                                  block_device_info=None):
        pass

    def rescue(self, context, instance, network_info, image_meta,
               rescue_password, block_device_info):
        pass

    def unrescue(
        self,
        context: nova_context.RequestContext,
        instance: 'objects.Instance',
    ):
        self.instances[instance.uuid].state = power_state.RUNNING

    def poll_rebooting_instances(self, timeout, instances):
        pass

    def migrate_disk_and_power_off(self, context, instance, dest,
                                   flavor, network_info,
                                   block_device_info=None,
                                   timeout=0, retry_interval=0):
        pass

    def finish_revert_migration(self, context, instance, network_info,
                                migration, block_device_info=None,
                                power_on=True):
        state = power_state.RUNNING if power_on else power_state.SHUTDOWN
        self.instances[instance.uuid] = FakeInstance(
            instance.name, state, instance.uuid)

    def post_live_migration_at_destination(self, context, instance,
                                           network_info,
                                           block_migration=False,
                                           block_device_info=None):
        # Called from the destination host after a successful live migration
        # so spawn the instance on this host to track it properly.
        image_meta = injected_files = admin_password = allocations = None
        self.spawn(context, instance, image_meta, injected_files,
                   admin_password, allocations)

    def power_off(self, instance, timeout=0, retry_interval=0):
        if instance.uuid in self.instances:
            self.instances[instance.uuid].state = power_state.SHUTDOWN
        else:
            raise exception.InstanceNotFound(instance_id=instance.uuid)

    def power_on(self, context, instance, network_info,
                 block_device_info=None, accel_info=None):
        if instance.uuid in self.instances:
            self.instances[instance.uuid].state = power_state.RUNNING
        else:
            raise exception.InstanceNotFound(instance_id=instance.uuid)

    def trigger_crash_dump(self, instance):
        pass

    def soft_delete(self, instance):
        pass

    def restore(self, instance):
        pass

    def pause(self, instance):
        pass

    def unpause(self, instance):
        pass

    def suspend(self, context, instance):
        pass

    def resume(self, context, instance, network_info, block_device_info=None):
        pass

    def destroy(self, context, instance, network_info, block_device_info=None,
                destroy_disks=True, destroy_secrets=True):
        self.unplug_vifs(instance, network_info)
        key = instance.uuid
        if key in self.instances:
            flavor = instance.flavor
            self.resources.release(
                vcpus=flavor.vcpus,
                mem=flavor.memory_mb,
                disk=flavor.root_gb)
            del self.instances[key]
        else:
            LOG.warning("fake Key '%(key)s' not in instances '%(inst)s'",
                        {'key': key,
                         'inst': self.instances}, instance=instance)

    def cleanup(self, context, instance, network_info, block_device_info=None,
                destroy_disks=True, migrate_data=None, destroy_vifs=True,
                destroy_secrets=True):
        # cleanup() should not be called when the guest has not been destroyed.
        if instance.uuid in self.instances:
            raise exception.InstanceExists(
                "Instance %s has not been destroyed." % instance.uuid)

    def attach_volume(self, context, connection_info, instance, mountpoint,
                      disk_bus=None, device_type=None, encryption=None):
        """Attach the disk to the instance at mountpoint using info."""
        instance_name = instance.name
        if instance_name not in self._mounts:
            self._mounts[instance_name] = {}
        self._mounts[instance_name][mountpoint] = connection_info

    def detach_volume(self, context, connection_info, instance, mountpoint,
                      encryption=None):
        """Detach the disk attached to the instance."""
        try:
            del self._mounts[instance.name][mountpoint]
        except KeyError:
            pass

    def swap_volume(self, context, old_connection_info, new_connection_info,
                    instance, mountpoint, resize_to):
        """Replace the disk attached to the instance."""
        instance_name = instance.name
        if instance_name not in self._mounts:
            self._mounts[instance_name] = {}
        self._mounts[instance_name][mountpoint] = new_connection_info

    def extend_volume(self, context, connection_info, instance,
                      requested_size):
        """Extend the disk attached to the instance."""
        pass

    def attach_interface(self, context, instance, image_meta, vif):
        if vif['id'] in self._interfaces:
            raise exception.InterfaceAttachFailed(
                    instance_uuid=instance.uuid)
        self._interfaces[vif['id']] = vif

    def detach_interface(self, context, instance, vif):
        try:
            del self._interfaces[vif['id']]
        except KeyError:
            raise exception.InterfaceDetachFailed(
                    instance_uuid=instance.uuid)

    def get_info(self, instance, use_cache=True):
        if instance.uuid not in self.instances:
            raise exception.InstanceNotFound(instance_id=instance.uuid)
        i = self.instances[instance.uuid]
        return hardware.InstanceInfo(state=i.state)

    def get_diagnostics(self, instance):
        return {'cpu0_time': 17300000000,
                'memory': 524288,
                'vda_errors': -1,
                'vda_read': 262144,
                'vda_read_req': 112,
                'vda_write': 5778432,
                'vda_write_req': 488,
                'vnet1_rx': 2070139,
                'vnet1_rx_drop': 0,
                'vnet1_rx_errors': 0,
                'vnet1_rx_packets': 26701,
                'vnet1_tx': 140208,
                'vnet1_tx_drop': 0,
                'vnet1_tx_errors': 0,
                'vnet1_tx_packets': 662,
        }

    def get_instance_diagnostics(self, instance):
        diags = diagnostics_obj.Diagnostics(
            state='running', driver='libvirt', hypervisor='kvm',
            hypervisor_os='ubuntu', uptime=46664, config_drive=True)
        diags.add_cpu(id=0, time=17300000000, utilisation=15)
        diags.add_nic(mac_address='01:23:45:67:89:ab',
                      rx_octets=2070139,
                      rx_errors=100,
                      rx_drop=200,
                      rx_packets=26701,
                      rx_rate=300,
                      tx_octets=140208,
                      tx_errors=400,
                      tx_drop=500,
                      tx_packets = 662,
                      tx_rate=600)
        diags.add_disk(read_bytes=262144,
                       read_requests=112,
                       write_bytes=5778432,
                       write_requests=488,
                       errors_count=1)
        diags.memory_details = diagnostics_obj.MemoryDiagnostics(
            maximum=524288, used=0)
        return diags

    def get_all_volume_usage(self, context, compute_host_bdms):
        """Return usage info for volumes attached to vms on
           a given host.
        """
        volusage = []
        if compute_host_bdms:
            volusage = [{'volume': compute_host_bdms[0][
                                       'instance_bdms'][0]['volume_id'],
                         'instance': compute_host_bdms[0]['instance'],
                         'rd_bytes': 0,
                         'rd_req': 0,
                         'wr_bytes': 0,
                         'wr_req': 0}]

        return volusage

    def get_host_cpu_stats(self):
        stats = {'kernel': 5664160000000,
                'idle': 1592705190000000,
                'user': 26728850000000,
                'iowait': 6121490000000}
        stats['frequency'] = 800
        return stats

    def block_stats(self, instance, disk_id):
        return [0, 0, 0, 0, None]

    def get_console_output(self, context, instance):
        return 'FAKE CONSOLE OUTPUT\nANOTHER\nLAST LINE'

    def get_vnc_console(self, context, instance):
        return ctype.ConsoleVNC(internal_access_path='FAKE',
                                host='fakevncconsole.com',
                                port=6969)

    def get_spice_console(self, context, instance):
        return ctype.ConsoleSpice(internal_access_path='FAKE',
                                  host='fakespiceconsole.com',
                                  port=6969,
                                  tlsPort=6970)

    def get_rdp_console(self, context, instance):
        return ctype.ConsoleRDP(internal_access_path='FAKE',
                                host='fakerdpconsole.com',
                                port=6969)

    def get_serial_console(self, context, instance):
        return ctype.ConsoleSerial(internal_access_path='FAKE',
                                   host='fakerdpconsole.com',
                                   port=6969)

    def get_mks_console(self, context, instance):
        return ctype.ConsoleMKS(internal_access_path='FAKE',
                                host='fakemksconsole.com',
                                port=6969)

    def get_available_resource(self, nodename):
        """Updates compute manager resource info on ComputeNode table.

           Since we don't have a real hypervisor, pretend we have lots of
           disk and ram.
        """
        cpu_info = collections.OrderedDict([
            ('arch', 'x86_64'),
            ('model', 'Nehalem'),
            ('vendor', 'Intel'),
            ('features', ['pge', 'clflush']),
            ('topology', {
                'cores': 1,
                'threads': 1,
                'sockets': 4,
                }),
            ])
        if nodename not in self.get_available_nodes():
            return {}

        host_status = self.host_status_base.copy()
        host_status.update(self.resources.dump())
        host_status['hypervisor_hostname'] = nodename
        host_status['host_hostname'] = nodename
        host_status['host_name_label'] = nodename
        host_status['cpu_info'] = jsonutils.dumps(cpu_info)
        return host_status

    def update_provider_tree(self, provider_tree, nodename, allocations=None):
        # NOTE(yikun): If the inv record does not exists, the allocation_ratio
        # will use the CONF.xxx_allocation_ratio value if xxx_allocation_ratio
        # is set, and fallback to use the initial_xxx_allocation_ratio
        # otherwise.
        inv = provider_tree.data(nodename).inventory
        ratios = self._get_allocation_ratios(inv)
        inventory = {
            'VCPU': {
                'total': self.vcpus,
                'min_unit': 1,
                'max_unit': self.vcpus,
                'step_size': 1,
                'allocation_ratio': ratios[orc.VCPU],
                'reserved': CONF.reserved_host_cpus,
            },
            'MEMORY_MB': {
                'total': self.memory_mb,
                'min_unit': 1,
                'max_unit': self.memory_mb,
                'step_size': 1,
                'allocation_ratio': ratios[orc.MEMORY_MB],
                'reserved': CONF.reserved_host_memory_mb,
            },
            'DISK_GB': {
                'total': self.local_gb,
                'min_unit': 1,
                'max_unit': self.local_gb,
                'step_size': 1,
                'allocation_ratio': ratios[orc.DISK_GB],
                'reserved': self._get_reserved_host_disk_gb_from_config(),
            },
        }
        provider_tree.update_inventory(nodename, inventory)

    def get_instance_disk_info(self, instance, block_device_info=None):
        return

    def live_migration(self, context, instance, dest,
                       post_method, recover_method, block_migration=False,
                       migrate_data=None):
        post_method(context, instance, dest, block_migration,
                            migrate_data)
        return

    def live_migration_force_complete(self, instance):
        return

    def live_migration_abort(self, instance):
        return

    def cleanup_live_migration_destination_check(self, context,
                                                 dest_check_data):
        return

    def check_can_live_migrate_destination(self, context, instance,
                                           src_compute_info, dst_compute_info,
                                           block_migration=False,
                                           disk_over_commit=False):
        data = migrate_data.LibvirtLiveMigrateData()
        data.filename = 'fake'
        data.image_type = CONF.libvirt.images_type
        data.graphics_listen_addr_vnc = CONF.vnc.server_listen
        data.graphics_listen_addr_spice = CONF.spice.server_listen
        data.serial_listen_addr = None
        # Notes(eliqiao): block_migration and disk_over_commit are not
        # nullable, so just don't set them if they are None
        if block_migration is not None:
            data.block_migration = block_migration
        if disk_over_commit is not None:
            data.disk_over_commit = disk_over_commit
        data.disk_available_mb = 100000
        data.is_shared_block_storage = True
        data.is_shared_instance_path = True

        return data

    def check_can_live_migrate_source(self, context, instance,
                                      dest_check_data, block_device_info=None):
        return dest_check_data

    def finish_migration(self, context, migration, instance, disk_info,
                         network_info, image_meta, resize_instance,
                         allocations, block_device_info=None, power_on=True):
        injected_files = admin_password = None
        # Finish migration is just like spawning the guest on a destination
        # host during resize/cold migrate, so re-use the spawn() fake to
        # claim resources and track the instance on this "hypervisor".
        self.spawn(context, instance, image_meta, injected_files,
                   admin_password, allocations,
                   block_device_info=block_device_info, power_on=power_on)

    def confirm_migration(self, context, migration, instance, network_info):
        # Confirm migration cleans up the guest from the source host so just
        # destroy the guest to remove it from the list of tracked instances
        # unless it is a same-host resize.
        if migration.source_compute != migration.dest_compute:
            self.destroy(context, instance, network_info)

    def pre_live_migration(self, context, instance, block_device_info,
                           network_info, disk_info, migrate_data):
        return migrate_data

    def rollback_live_migration_at_destination(self, context, instance,
                                               network_info,
                                               block_device_info,
                                               destroy_disks=True,
                                               migrate_data=None):
        return

    def _test_remove_vm(self, instance_uuid):
        """Removes the named VM, as if it crashed. For testing."""
        self.instances.pop(instance_uuid)

    def host_power_action(self, action):
        """Reboots, shuts down or powers up the host."""
        return action

    def host_maintenance_mode(self, host, mode):
        """Start/Stop host maintenance window. On start, it triggers
        guest VMs evacuation.
        """
        if not mode:
            return 'off_maintenance'
        return 'on_maintenance'

    def set_host_enabled(self, enabled):
        """Sets the specified host's ability to accept new instances."""
        if enabled:
            return 'enabled'
        return 'disabled'

    def get_volume_connector(self, instance):
        return {'ip': CONF.my_block_storage_ip,
                'initiator': 'fake',
                'host': self._host}

    def get_available_nodes(self, refresh=False):
        return self._nodes

    def instance_on_disk(self, instance):
        return False

    def quiesce(self, context, instance, image_meta):
        pass

    def unquiesce(self, context, instance, image_meta):
        pass


class FakeVirtAPI(virtapi.VirtAPI):
    @contextlib.contextmanager
    def wait_for_instance_event(self, instance, event_names, deadline=300,
                                error_callback=None):
        # NOTE(danms): Don't actually wait for any events, just
        # fall through
        yield

    def exit_wait_early(self, events):
        # We never wait, so there is nothing to exit early
        pass

    def update_compute_provider_status(self, context, rp_uuid, enabled):
        pass


class SmallFakeDriver(FakeDriver):
    # The api samples expect specific cpu memory and disk sizes. In order to
    # allow the FakeVirt driver to be used outside of the unit tests, provide
    # a separate class that has the values expected by the api samples. So
    # instead of requiring new samples every time those
    # values are adjusted allow them to be overwritten here.

    vcpus = 2
    memory_mb = 8192
    local_gb = 1028


class MediumFakeDriver(FakeDriver):
    # Fake driver that has enough resources to host more than one instance
    # but not that much that cannot be exhausted easily

    vcpus = 10
    memory_mb = 8192
    local_gb = 1028


class SameHostColdMigrateDriver(MediumFakeDriver):
    """MediumFakeDriver variant that supports same-host cold migrate."""
    capabilities = dict(FakeDriver.capabilities,
                        supports_migrate_to_same_host=True)


class RescueBFVDriver(MediumFakeDriver):
    capabilities = dict(FakeDriver.capabilities, supports_bfv_rescue=True)


class PowerUpdateFakeDriver(SmallFakeDriver):
    # A specific fake driver for the power-update external event testing.

    def __init__(self, virtapi):
        super(PowerUpdateFakeDriver, self).__init__(virtapi=None)
        self.driver = ironic.IronicDriver(virtapi=virtapi)

    def power_update_event(self, instance, target_power_state):
        """Update power state of the specified instance in the nova DB."""
        self.driver.power_update_event(instance, target_power_state)


class MediumFakeDriverWithNestedCustomResources(MediumFakeDriver):
    # A MediumFakeDriver variant that also reports CUSTOM_MAGIC resources on
    # a nested resource provider
    vcpus = 10
    memory_mb = 8192
    local_gb = 1028
    child_resources = {
            'CUSTOM_MAGIC': {
                'total': 10,
                'reserved': 0,
                'min_unit': 1,
                'max_unit': 10,
                'step_size': 1,
                'allocation_ratio': 1,
            }
    }

    def update_provider_tree(self, provider_tree, nodename, allocations=None):
        super(
            MediumFakeDriverWithNestedCustomResources,
            self).update_provider_tree(
                provider_tree, nodename,
                allocations=allocations)

        if not provider_tree.exists(nodename + '-child'):
            provider_tree.new_child(name=nodename + '-child',
                                    parent=nodename)

        provider_tree.update_inventory(nodename + '-child',
                                       self.child_resources)


class FakeFinishMigrationFailDriver(FakeDriver):
    """FakeDriver variant that will raise an exception from finish_migration"""

    def finish_migration(self, *args, **kwargs):
        raise exception.VirtualInterfaceCreateException()


class PredictableNodeUUIDDriver(SmallFakeDriver):
    """SmallFakeDriver variant that reports a predictable node uuid in
    get_available_resource, like IronicDriver.
    """

    def get_available_resource(self, nodename):
        resources = super(
            PredictableNodeUUIDDriver, self).get_available_resource(nodename)
        # This is used in ComputeNode.update_from_virt_driver which is called
        # from the ResourceTracker when creating a ComputeNode.
        resources['uuid'] = uuid.uuid5(uuid.NAMESPACE_DNS, nodename)
        return resources


class FakeRescheduleDriver(FakeDriver):
    """FakeDriver derivative that triggers a reschedule on the first spawn
    attempt. This is expected to only be used in tests that have more than
    one compute service.
    """
    # dict, keyed by instance uuid, mapped to a boolean telling us if the
    # instance has been rescheduled or not
    rescheduled = {}

    def spawn(self, context, instance, image_meta, injected_files,
              admin_password, allocations, network_info=None,
              block_device_info=None, power_on=True, accel_info=None):
        if not self.rescheduled.get(instance.uuid, False):
            # We only reschedule on the first time something hits spawn().
            self.rescheduled[instance.uuid] = True
            raise exception.ComputeResourcesUnavailable(
                reason='FakeRescheduleDriver')
        super(FakeRescheduleDriver, self).spawn(
            context, instance, image_meta, injected_files,
            admin_password, allocations, network_info, block_device_info,
            power_on)


class FakeRescheduleDriverWithNestedCustomResources(
        FakeRescheduleDriver, MediumFakeDriverWithNestedCustomResources):
    pass


class FakeBuildAbortDriver(FakeDriver):
    """FakeDriver derivative that always fails on spawn() with a
    BuildAbortException so no reschedule is attempted.
    """

    def spawn(self, context, instance, image_meta, injected_files,
              admin_password, allocations, network_info=None,
              block_device_info=None, power_on=True, accel_info=None):
        raise exception.BuildAbortException(
            instance_uuid=instance.uuid, reason='FakeBuildAbortDriver')


class FakeBuildAbortDriverWithNestedCustomResources(
    FakeBuildAbortDriver, MediumFakeDriverWithNestedCustomResources):
    pass


class FakeUnshelveSpawnFailDriver(FakeDriver):
    """FakeDriver derivative that always fails on spawn() with a
    VirtualInterfaceCreateException when unshelving an offloaded instance.
    """

    def spawn(self, context, instance, image_meta, injected_files,
              admin_password, allocations, network_info=None,
              block_device_info=None, power_on=True, accel_info=None):
        if instance.vm_state == vm_states.SHELVED_OFFLOADED:
            raise exception.VirtualInterfaceCreateException(
                'FakeUnshelveSpawnFailDriver')
        # Otherwise spawn normally during the initial build.
        super(FakeUnshelveSpawnFailDriver, self).spawn(
            context, instance, image_meta, injected_files,
            admin_password, allocations, network_info, block_device_info,
            power_on)


class FakeUnshelveSpawnFailDriverWithNestedCustomResources(
    FakeUnshelveSpawnFailDriver, MediumFakeDriverWithNestedCustomResources):
    pass


class FakeLiveMigrateDriver(FakeDriver):
    """FakeDriver derivative to handle force_complete and abort calls.

    This module serves those tests that need to abort or force-complete
    the live migration, thus the live migration will never be finished
    without the force_complete_migration or delete_migration API calls.

    """

    def __init__(self, virtapi, read_only=False):
        super(FakeLiveMigrateDriver, self).__init__(virtapi, read_only)
        self._migrating = True
        self._abort_migration = True

    def live_migration(self, context, instance, dest,
                       post_method, recover_method, block_migration=False,
                       migrate_data=None):
        self._abort_migration = False
        self._migrating = True
        count = 0
        while self._migrating and count < 50:
            time.sleep(0.1)
            count = count + 1

        if self._abort_migration:
            recover_method(context, instance, dest, migrate_data,
                           migration_status='cancelled')
        else:
            post_method(context, instance, dest, block_migration,
                        migrate_data)

    def live_migration_force_complete(self, instance):
        self._migrating = False
        if instance.uuid in self.instances:
            del self.instances[instance.uuid]

    def live_migration_abort(self, instance):
        self._abort_migration = True
        self._migrating = False

    def post_live_migration(self, context, instance, block_device_info,
                            migrate_data=None):
        # Runs on the source host, called from
        # ComputeManager._post_live_migration so just delete the instance
        # from being tracked on the source host.
        self.destroy(context, instance, network_info=None,
                     block_device_info=block_device_info)


class FakeLiveMigrateDriverWithNestedCustomResources(
        FakeLiveMigrateDriver, MediumFakeDriverWithNestedCustomResources):
    pass


class FakeDriverWithPciResources(SmallFakeDriver):

    PCI_ADDR_PF1 = '0000:01:00.0'
    PCI_ADDR_PF1_VF1 = '0000:01:00.1'
    PCI_ADDR_PF2 = '0000:02:00.0'
    PCI_ADDR_PF2_VF1 = '0000:02:00.1'
    PCI_ADDR_PF3 = '0000:03:00.0'
    PCI_ADDR_PF3_VF1 = '0000:03:00.1'

    # NOTE(gibi): Always use this fixture along with the
    # FakeDriverWithPciResources to make the necessary configuration for the
    # driver.
    class FakeDriverWithPciResourcesConfigFixture(fixtures.Fixture):
        def setUp(self):
            super(FakeDriverWithPciResources.
                  FakeDriverWithPciResourcesConfigFixture, self).setUp()
            # Set passthrough_whitelist before the compute node starts to match
            # with the PCI devices reported by this fake driver.

            # NOTE(gibi): 0000:01:00 is tagged to physnet1 and therefore not a
            # match based on physnet to our sriov port
            # 'port_with_sriov_resource_request' as the network of that port
            # points to physnet2 with the attribute
            # 'provider:physical_network'. Nova pci handling already enforces
            # this rule.
            #
            # 0000:02:00 and 0000:03:00 are both tagged to physnet2 and
            # therefore a good match for our sriov port based on physnet.
            # Having two PFs on the same physnet will allow us to test the
            # placement allocation - physical allocation matching based on the
            # bandwidth allocation in the future.
            CONF.set_override('passthrough_whitelist', override=[
                jsonutils.dumps(
                    {
                        "address": {
                            "domain": "0000",
                            "bus": "01",
                            "slot": "00",
                            "function": ".*"},
                        "physical_network": "physnet1",
                    }
                ),
                jsonutils.dumps(
                    {
                        "address": {
                            "domain": "0000",
                            "bus": "02",
                            "slot": "00",
                            "function": ".*"},
                        "physical_network": "physnet2",
                    }
                ),
                jsonutils.dumps(
                    {
                        "address": {
                            "domain": "0000",
                            "bus": "03",
                            "slot": "00",
                            "function": ".*"},
                        "physical_network": "physnet2",
                    }
                ),
            ],
                             group='pci')

            self.useFixture(fixtures.MockPatch(
                'nova.pci.utils.get_mac_by_pci_address',
                return_value='52:54:00:1e:59:c6'))

            self.useFixture(fixtures.MockPatch(
                'nova.pci.utils.get_vf_num_by_pci_address',
                return_value=1))

    def get_available_resource(self, nodename):
        host_status = super(
            FakeDriverWithPciResources, self).get_available_resource(nodename)
        # 01:00.0 - PF - ens1
        #  |---- 01:00.1 - VF
        #
        # 02:00.0 - PF - ens2
        #  |---- 02:00.1 - VF
        #
        # 03:00.0 - PF - ens3
        #  |---- 03:00.1 - VF
        host_status['pci_passthrough_devices'] = jsonutils.dumps([
            {
                'address': self.PCI_ADDR_PF1,
                'product_id': 'fake-product_id',
                'vendor_id': 'fake-vendor_id',
                'status': 'available',
                'dev_type': 'type-PF',
                'parent_addr': None,
                'numa_node': 0,
                'label': 'fake-label',
            },
            {
                'address': self.PCI_ADDR_PF1_VF1,
                'product_id': 'fake-product_id',
                'vendor_id': 'fake-vendor_id',
                'status': 'available',
                'dev_type': 'type-VF',
                'parent_addr': self.PCI_ADDR_PF1,
                'numa_node': 0,
                'label': 'fake-label',
                "parent_ifname": self._host + "-ens1",
            },
            {
                'address': self.PCI_ADDR_PF2,
                'product_id': 'fake-product_id',
                'vendor_id': 'fake-vendor_id',
                'status': 'available',
                'dev_type': 'type-PF',
                'parent_addr': None,
                'numa_node': 0,
                'label': 'fake-label',
            },
            {
                'address': self.PCI_ADDR_PF2_VF1,
                'product_id': 'fake-product_id',
                'vendor_id': 'fake-vendor_id',
                'status': 'available',
                'dev_type': 'type-VF',
                'parent_addr': self.PCI_ADDR_PF2,
                'numa_node': 0,
                'label': 'fake-label',
                "parent_ifname": self._host + "-ens2",
            },
            {
                'address': self.PCI_ADDR_PF3,
                'product_id': 'fake-product_id',
                'vendor_id': 'fake-vendor_id',
                'status': 'available',
                'dev_type': 'type-PF',
                'parent_addr': None,
                'numa_node': 0,
                'label': 'fake-label',
            },
            {
                'address': self.PCI_ADDR_PF3_VF1,
                'product_id': 'fake-product_id',
                'vendor_id': 'fake-vendor_id',
                'status': 'available',
                'dev_type': 'type-VF',
                'parent_addr': self.PCI_ADDR_PF3,
                'numa_node': 0,
                'label': 'fake-label',
                "parent_ifname": self._host + "-ens3",
            },
        ])
        return host_status


class FakeLiveMigrateDriverWithPciResources(
        FakeLiveMigrateDriver, FakeDriverWithPciResources):
    """FakeDriver derivative to handle force_complete and abort calls.

    This module serves those tests that need to abort or force-complete
    the live migration, thus the live migration will never be finished
    without the force_complete_migration or delete_migration API calls.

    """


class FakeDriverWithCaching(FakeDriver):
    def __init__(self, *a, **k):
        super(FakeDriverWithCaching, self).__init__(*a, **k)
        self.cached_images = set()

    def cache_image(self, context, image_id):
        if image_id in self.cached_images:
            return False
        else:
            self.cached_images.add(image_id)
            return True
